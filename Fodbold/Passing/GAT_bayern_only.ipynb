{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "import torch_geometric.nn \n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch_geometric.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_graphs = pd.read_pickle('/Users/morten/Desktop/p5 kode/5-semester/Momentum graphs Bayern only.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = []\n",
    "delete_idx = []\n",
    "for idx, graph in pkl_graphs.items():\n",
    "    if idx.endswith('45'):\n",
    "        delete_idx.append(idx)\n",
    "        continue\n",
    "    temp = [node for node in graph.nodes() if node not in unique_nodes]\n",
    "    unique_nodes.extend(temp)\n",
    "for idx in delete_idx:\n",
    "    del pkl_graphs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_pos = dict(enumerate(unique_nodes))\n",
    "pos_to_idx = {pos : idx for idx, pos in idx_to_pos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[23, 26], edge_index=[2, 22], weight=[22])\n"
     ]
    }
   ],
   "source": [
    "pyg_data = []\n",
    "\n",
    "#add node attributes\n",
    "for graph in pkl_graphs.values():\n",
    "    filtered_edges = [(u, v) for u, v, d in graph.edges(data=True) if d['weight'] > 3]\n",
    "    filtered_graph = graph.edge_subgraph(filtered_edges)\n",
    "\n",
    "    closeness = nx.closeness_centrality(filtered_graph)\n",
    "    betweenness = nx.closeness_centrality(filtered_graph)\n",
    "    pagerank = nx.pagerank(graph, weight='weight')\n",
    "    centrality_list = [closeness, betweenness, pagerank] \n",
    "\n",
    "    adj_dict = nx.to_dict_of_dicts(graph)\n",
    "    \n",
    "    for node in list(graph.nodes()):\n",
    "        adj_vect = np.zeros((len(unique_nodes)))\n",
    "        players = adj_dict[node]\n",
    "        for key, value in players.items():\n",
    "            adj_vect[pos_to_idx[key]] = value['weight']\n",
    "        adj_vect = torch.from_numpy(adj_vect).float()\n",
    "        centrality_vect = []\n",
    "        for measure in centrality_list:\n",
    "            if node in list(measure.keys()):\n",
    "                centrality_vect.append(measure[node])\n",
    "            else:\n",
    "                centrality_vect.append(0)\n",
    "        centrality_vect = torch.Tensor(centrality_vect).float()        \n",
    "        graph.nodes[node]['x'] = torch.cat((adj_vect, centrality_vect), -1)\n",
    "\n",
    "    for node in unique_nodes:\n",
    "        if node not in graph.nodes:\n",
    "            graph.add_node(node) \n",
    "            graph.nodes[node]['x'] = torch.from_numpy(np.zeros((len(unique_nodes)+3))).float()  \n",
    "            \n",
    "\n",
    "    data = from_networkx(graph)\n",
    "    try:\n",
    "        data.momentum\n",
    "        pyg_data.append(data)\n",
    "    except:\n",
    "        print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_values = [data.momentum for data in pyg_data]\n",
    "\n",
    "momentum_min = min(momentum_values)\n",
    "momentum_max = max(momentum_values)\n",
    "\n",
    "for data in pyg_data:\n",
    "    normalized_momentum = (data.momentum - momentum_min) / (momentum_max - momentum_min)\n",
    "    \n",
    "    scaled_momentum = 2 * normalized_momentum - 1\n",
    "    data.momentum = scaled_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = random.sample(range(len(pyg_data)), int(len(pyg_data) * 0.8))\n",
    "test_idx = [i for i in range(len(pyg_data)) if i not in train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, alpha, input_dim, hidden_dim, output_dim, num_heads, dropout=0.4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        self.layer1 = GATConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)  # Reduced hidden_dim\n",
    "        self.layer3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)  # Further reduction\n",
    "        self.layer4 = GATConv(hidden_dim * num_heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        \n",
    "        self.activation_function = nn.ELU(alpha=alpha)\n",
    "        self.final_activation = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, input, edge_index):\n",
    "        output = self.layer1(input, edge_index)\n",
    "        output = self.activation_function(output)\n",
    "        output = F.dropout(output, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        output = self.layer2(output, edge_index)\n",
    "        output = self.activation_function(output)\n",
    "        output = F.dropout(output, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        output = self.layer3(output, edge_index)\n",
    "        output = self.activation_function(output)\n",
    "        output = F.dropout(output, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        output = self.layer4(output, edge_index)\n",
    "        \n",
    "        output = output.mean(dim=0)\n",
    "        output = self.final_activation(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(unique_nodes)+3\n",
    "lr = 0.001\n",
    "\n",
    "gat = GAT(alpha=0.005, input_dim = input_dim, hidden_dim = 100, output_dim = 1, num_heads = 5)\n",
    "optimizer = torch.optim.SGD(gat.parameters(), lr=lr, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "epochs_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/Users/morten/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 431/431 [00:01<00:00, 215.78it/s]\n",
      "  1%|          | 1/100 [00:01<03:17,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 409/431 [00:01<00:00, 230.78it/s]\n",
      "  1%|          | 1/100 [00:03<06:13,  3.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m gat(\u001b[38;5;28minput\u001b[39m, edge_idx)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, label)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[62], line 20\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, input, edge_index)\u001b[0m\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(output)\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(output, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(output, edge_index)\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(output)\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(output, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/gat_conv.py:335\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m (x_src, x_dst)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Next, we compute node-level attention coefficients, both for source\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# and target nodes (if present):\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m alpha_src \u001b[38;5;241m=\u001b[39m (x_src \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_src)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    336\u001b[0m alpha_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x_dst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (x_dst \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_dst)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    337\u001b[0m alpha \u001b[38;5;241m=\u001b[39m (alpha_src, alpha_dst)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.tqdm(range(epochs_num)):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for idx in tqdm.tqdm(range(len(train_idx))):\n",
    "        \n",
    "        input = pyg_data[idx].x\n",
    "        edge_idx = pyg_data[idx].edge_index\n",
    "        label = pyg_data[idx]['momentum']\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = gat(input, edge_idx)\n",
    "        \n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gat.parameters(), max_norm=1.0)\n",
    "\n",
    "        for p in gat.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-lr)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_idx):.4f}\") \n",
    "gat.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in test_idx:\n",
    "        output = gat(pyg_data[idx].x, pyg_data[idx].edge_index)\n",
    "        y_pred.append(output.numpy())\n",
    "        y_true.append(pyg_data[idx].momentum.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28043282"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_true, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
